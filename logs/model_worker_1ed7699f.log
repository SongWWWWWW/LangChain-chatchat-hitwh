2024-04-16 19:34:45 | INFO | model_worker | Loading the model ['chatglm3-6b'] on worker 1ed7699f ...
2024-04-16 19:35:35 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                      | 0/7 [00:00<?, ?it/s]
2024-04-16 19:35:39 | ERROR | stderr | Loading checkpoint shards:  14%|████████▊                                                     | 1/7 [00:04<00:24,  4.10s/it]
2024-04-16 19:35:44 | ERROR | stderr | Loading checkpoint shards:  29%|█████████████████▋                                            | 2/7 [00:08<00:21,  4.29s/it]
2024-04-16 19:35:48 | ERROR | stderr | Loading checkpoint shards:  43%|██████████████████████████▌                                   | 3/7 [00:13<00:17,  4.38s/it]
2024-04-16 19:35:52 | ERROR | stderr | Loading checkpoint shards:  57%|███████████████████████████████████▍                          | 4/7 [00:17<00:12,  4.25s/it]
2024-04-16 19:35:57 | ERROR | stderr | Loading checkpoint shards:  71%|████████████████████████████████████████████▎                 | 5/7 [00:21<00:08,  4.31s/it]
2024-04-16 19:36:01 | ERROR | stderr | Loading checkpoint shards:  86%|█████████████████████████████████████████████████████▏        | 6/7 [00:25<00:04,  4.32s/it]
2024-04-16 19:36:04 | ERROR | stderr | Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:28<00:00,  3.70s/it]
2024-04-16 19:36:04 | ERROR | stderr | Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:28<00:00,  4.03s/it]
2024-04-16 19:36:04 | ERROR | stderr | 
2024-04-16 19:36:15 | ERROR | stderr | Process model_worker - chatglm3-6b:
2024-04-16 19:36:15 | ERROR | stderr | Traceback (most recent call last):
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
2024-04-16 19:36:15 | ERROR | stderr |     self.run()
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/process.py", line 108, in run
2024-04-16 19:36:15 | ERROR | stderr |     self._target(*self._args, **self._kwargs)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/wcc/Langchain-Chatchat/startup.py", line 386, in run_model_worker
2024-04-16 19:36:15 | ERROR | stderr |     app = create_model_worker_app(log_level=log_level, **kwargs)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/wcc/Langchain-Chatchat/startup.py", line 214, in create_model_worker_app
2024-04-16 19:36:15 | ERROR | stderr |     worker = ModelWorker(
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/fastchat/serve/model_worker.py", line 77, in __init__
2024-04-16 19:36:15 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/fastchat/model/model_adapter.py", line 367, in load_model
2024-04-16 19:36:15 | ERROR | stderr |     model.to(device)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2460, in to
2024-04-16 19:36:15 | ERROR | stderr |     return super().to(*args, **kwargs)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
2024-04-16 19:36:15 | ERROR | stderr |     return self._apply(convert)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
2024-04-16 19:36:15 | ERROR | stderr |     module._apply(fn)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
2024-04-16 19:36:15 | ERROR | stderr |     module._apply(fn)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
2024-04-16 19:36:15 | ERROR | stderr |     module._apply(fn)
2024-04-16 19:36:15 | ERROR | stderr |   [Previous line repeated 3 more times]
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
2024-04-16 19:36:15 | ERROR | stderr |     param_applied = fn(param)
2024-04-16 19:36:15 | ERROR | stderr |   File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
2024-04-16 19:36:15 | ERROR | stderr |     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2024-04-16 19:36:15 | ERROR | stderr | torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacty of 47.53 GiB of which 3.06 MiB is free. Process 194989 has 38.69 GiB memory in use. Including non-PyTorch memory, this process has 8.82 GiB memory in use. Of the allocated memory 8.56 GiB is allocated by PyTorch, and 1.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
