CPU architecture: x86_64

正在检测订阅地址...
Clash订阅地址可访问！ [60G[[1;32m  OK  [0;39m]

正在下载Clash配置文件...
配置文件config.yaml下载成功！ [60G[[1;32m  OK  [0;39m]

判断订阅内容是否符合clash配置文件标准:
解码后的内容不符合clash标准，尝试将其转换为标准格式
配置文件已成功转换成clash标准格式

正在启动Clash服务...
服务启动成功！ [60G[[1;32m  OK  [0;39m]

Clash Dashboard 访问地址: http://<ip>:9090/ui
Secret: b2967b1d14b3de70f2ea64a4afc1c3561f4a48951670e538b11a75236f7e5649

/home/root1/clash-for-linux/start.sh: 行 187: /etc/profile.d/clash.sh: 权限不够
请执行以下命令加载环境变量: source /etc/profile.d/clash.sh

请执行以下命令开启系统代理: proxy_on

若要临时关闭系统代理，请执行: proxy_off

[32m[√] 已开启代理[0m
--2024-01-09 21:13:33--  http://google.com/
正在连接 127.0.0.1:7890... 已连接。
已发出 Proxy 请求，正在等待回应... 301 Moved Permanently
位置：http://www.google.com/ [跟随至新的 URL]
--2024-01-09 21:13:34--  http://www.google.com/
再次使用存在的到 127.0.0.1:7890 的连接。
已发出 Proxy 请求，正在等待回应... 200 OK
长度： 未指定 [text/html]
正在保存至: “index.html”

     0K .......... ........                                     572K=0.03s

2024-01-09 21:13:34 (572 KB/s) - “index.html” 已保存 [18636]

[32m[√] 已开启代理[0m
--2024-01-09 21:13:34--  http://google.com/
正在连接 127.0.0.1:7890... 已连接。
已发出 Proxy 请求，正在等待回应... 301 Moved Permanently
位置：http://www.google.com/ [跟随至新的 URL]
--2024-01-09 21:13:34--  http://www.google.com/
再次使用存在的到 127.0.0.1:7890 的连接。
已发出 Proxy 请求，正在等待回应... 200 OK
长度： 未指定 [text/html]
正在保存至: “index.html”

     0K .......... ........                                     225K=0.08s

2024-01-09 21:13:35 (225 KB/s) - “index.html” 已保存 [18633]

2024-01-09 21:13:40,676 - startup.py[line:650] - INFO: 正在启动服务：
2024-01-09 21:13:40,676 - startup.py[line:651] - INFO: 如需查看 llm_api 日志，请前往 /home/root1/wcc/Langchain-Chatchat/logs


==============================Langchain-Chatchat Configuration==============================
操作系统：Linux-5.15.0-91-generic-x86_64-with-glibc2.31.
python版本：3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]
项目版本：v0.2.8
langchain版本：0.0.344. fastchat版本：0.2.34


当前使用的分词器：ChineseRecursiveTextSplitter
当前启动的LLM模型：['chatglm3-6b', 'zhipu-api', 'openai-api'] @ cuda
{'device': 'cuda',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'model_path': 'THUDM/chatglm3-6b',
 'port': 20002}
{'api_key': '00f57a85b1e06e2c2d38b3a067698cc0.dTvbnzYTM6afe3M3',
 'device': 'auto',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'online_api': True,
 'port': 21001,
 'provider': 'ChatGLMWorker',
 'version': 'chatglm_turbo',
 'worker_class': <class 'server.model_workers.zhipu.ChatGLMWorker'>}
{'api_base_url': 'https://api.openai.com/v1',
 'api_key': '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',
 'device': 'auto',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'model_name': 'gpt-3.5-turbo',
 'online_api': True,
 'openai_proxy': '',
 'port': 20002}
当前Embbedings模型： bge-large-zh @ cuda
==============================Langchain-Chatchat Configuration==============================


2024-01-09 21:13:45 | INFO | model_worker | Register to controller
2024-01-09 21:13:45 | ERROR | stderr | INFO:     Started server process [355586]
2024-01-09 21:13:45 | ERROR | stderr | INFO:     Waiting for application startup.
2024-01-09 21:13:45 | ERROR | stderr | INFO:     Application startup complete.
2024-01-09 21:13:45 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:20000 (Press CTRL+C to quit)
2024-01-09 21:13:46 | INFO | model_worker | Loading the model ['chatglm3-6b'] on worker c4c2a2be ...
2024-01-09 21:13:49 | ERROR | stderr | Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]
2024-01-09 21:13:50 | ERROR | stderr | Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.84it/s]
2024-01-09 21:13:50 | ERROR | stderr | Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  2.80it/s]
2024-01-09 21:13:50 | ERROR | stderr | Loading checkpoint shards:  43%|████▎     | 3/7 [00:01<00:01,  2.85it/s]
2024-01-09 21:13:51 | ERROR | stderr | Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:01,  2.90it/s]
2024-01-09 21:13:51 | ERROR | stderr | Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  2.86it/s]
2024-01-09 21:13:51 | ERROR | stderr | Loading checkpoint shards:  86%|████████▌ | 6/7 [00:02<00:00,  2.85it/s]
2024-01-09 21:13:51 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.21it/s]
2024-01-09 21:13:51 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.00it/s]
2024-01-09 21:13:51 | ERROR | stderr | 
2024-01-09 21:13:54 | INFO | model_worker | Register to controller
INFO:     Started server process [356021]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:7861 (Press CTRL+C to quit)

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.


  You can now view your Streamlit app in your browser.

  URL: http://0.0.0.0:8501


Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.


  You can now view your Streamlit app in your browser.

  Network URL: http://172.26.94.25:8502
  External URL: http://61.227.239.195:8502

2024-01-09 21:14:37,558 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:46170 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:14:37,564 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-09 21:14:37,780 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:46170 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:14:37,785 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:46170 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-09 21:14:37,805 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-09 21:19:55,003 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37308 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:19:55,008 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-09 21:19:55,223 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37308 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:19:55,229 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37308 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-09 21:19:55,248 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-09 21:20:09,772 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37768 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:20:09,777 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-09 21:20:09,995 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37768 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:20:10,000 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37768 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-09 21:20:10,017 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-09 21:20:19,276 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:47582 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:20:19,281 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-09 21:20:19,390 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:47582 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-09 21:20:19,395 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:47582 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-09 21:20:19,420 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-11 17:09:30,102 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:60234 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-11 17:09:30,108 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-11 17:09:30,321 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:60234 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-11 17:09:30,325 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:60234 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-11 17:09:30,339 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-11 17:09:41,159 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:53640 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-11 17:09:41,164 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-11 17:09:41,378 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:53640 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-11 17:09:41,383 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:53640 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-11 17:09:41,400 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-11 17:09:53,205 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:34290 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-11 17:09:53,210 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-11 17:09:53,422 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:34290 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-11 17:09:53,428 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:34290 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-11 17:09:53,456 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-14 23:50:37,805 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:36308 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-14 23:50:37,845 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-14 23:50:38,053 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:36308 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-14 23:50:38,056 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:36308 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-14 23:50:38,135 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-14 23:50:43,607 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:55138 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-14 23:50:43,612 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-14 23:50:43,725 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:55138 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-14 23:50:43,729 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:55138 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-14 23:50:43,747 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:16:05,941 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:41930 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:05,959 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:06,172 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:41930 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:06,176 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:41930 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:16:06,210 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:16:15,303 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50500 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:15,308 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:15,417 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50500 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:15,421 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50500 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:16:15,436 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:16:15.445 Please replace `st.experimental_rerun` with `st.rerun`.

`st.experimental_rerun` will be removed after 2024-04-01.
2024-01-15 20:16:15,884 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50510 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:15,889 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:16,000 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50510 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:16,005 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50510 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:16:16,019 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:16:20,851 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50516 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:20,856 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:20.860 Please replace `st.experimental_rerun` with `st.rerun`.

`st.experimental_rerun` will be removed after 2024-04-01.
2024-01-15 20:16:21,291 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50518 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:21,296 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:21,406 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50518 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:21,410 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:50518 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:16:21,434 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43102 - "POST /llm_model/get_model_config HTTP/1.1" 200 OK
2024-01-15 20:16:44,081 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/get_model_config "HTTP/1.1 200 OK"
2024-01-15 20:16:44,356 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43108 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:44,361 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:44,464 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43108 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:44,468 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43108 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:16:44,482 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:16:49,810 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43116 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:49,815 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:16:49,922 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43116 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:16:49,927 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43116 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:16:49,943 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:43116 - "POST /chat/chat HTTP/1.1" 200 OK
2024-01-15 20:16:51,840 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat "HTTP/1.1 200 OK"
2024-01-15 20:16:53,324 - _client.py[line:1729] - INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-15 20:16:54.209 Uncaught app exception
Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 375, in image_to_url
    with open(image, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/root1/wcc/Langchain-Chatchat/coding/.png'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/memory_media_file_storage.py", line 165, in _read_file
    with open(filename, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/root1/wcc/Langchain-Chatchat/coding/.png'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 534, in _run_script
    exec(code, module.__dict__)
  File "/home/root1/wcc/Langchain-Chatchat/webui.py", line 105, in <module>
    pages[selected_page]["func"](api=api, is_lite=is_lite)
  File "/home/root1/wcc/Langchain-Chatchat/webui_pages/dialogue/dialogue.py", line 430, in dialogue_page
    st.image(image_path, caption=" LLM's Image", use_column_width=True)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/metrics_util.py", line 396, in wrapped_func
    result = non_optional_func(*args, **kwargs)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 165, in image
    marshall_images(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 534, in marshall_images
    proto_img.url = image_to_url(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 385, in image_to_url
    url = runtime.get_instance().media_file_mgr.add(image, mimetype, image_id)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/media_file_manager.py", line 222, in add
    file_id = self._storage.load_and_get_id(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/memory_media_file_storage.py", line 116, in load_and_get_id
    file_data = self._read_file(path_or_data)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/memory_media_file_storage.py", line 168, in _read_file
    raise MediaFileStorageError(f"Error opening '{filename}'") from ex
streamlit.runtime.media_file_storage.MediaFileStorageError: Error opening '/home/root1/wcc/Langchain-Chatchat/coding/.png'
2024-01-15 20:27:46,393 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:56836 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:27:46,399 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:27:46,612 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:56836 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:27:46,617 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:56836 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:27:46,633 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:28:17,229 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33036 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:28:17,235 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:28:17,343 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33036 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:28:17,348 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33036 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:28:17,367 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33036 - "POST /chat/chat HTTP/1.1" 200 OK
2024-01-15 20:28:17,604 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat "HTTP/1.1 200 OK"
2024-01-15 20:28:18 | INFO | stdout | INFO:     127.0.0.1:39302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2024-01-15 20:28:18,242 - _client.py[line:1729] - INFO: HTTP Request: POST http://127.0.0.1:20000/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-15 20:28:18 | INFO | httpx | HTTP Request: POST http://127.0.0.1:21001/worker_generate_stream "HTTP/1.1 200 OK"
2024-01-15 20:28:18,433 - utils.py[line:24] - ERROR: object of type 'NoneType' has no len()
Traceback (most recent call last):
  File "/home/root1/wcc/Langchain-Chatchat/server/utils.py", line 22, in wrap_done
    await fn
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chains/base.py", line 381, in acall
    raise e
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chains/base.py", line 375, in acall
    await self._acall(inputs, run_manager=run_manager)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chains/llm.py", line 275, in _acall
    response = await self.agenerate([inputs], run_manager=run_manager)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chains/llm.py", line 142, in agenerate
    return await self.llm.agenerate_prompt(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 501, in agenerate_prompt
    return await self.agenerate(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 461, in agenerate
    raise exceptions[0]
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 564, in _agenerate_with_cache
    return await self._agenerate(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chat_models/openai.py", line 506, in _agenerate
    return await agenerate_from_stream(stream_iter)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 81, in agenerate_from_stream
    async for chunk in stream:
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain/chat_models/openai.py", line 477, in _astream
    if len(chunk["choices"]) == 0:
TypeError: object of type 'NoneType' has no len()
2024-01-15 20:28:18,446 - utils.py[line:27] - ERROR: TypeError: Caught exception: object of type 'NoneType' has no len()
2024-01-15 20:28:18.448 Uncaught app exception
Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 375, in image_to_url
    with open(image, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/root1/wcc/Langchain-Chatchat/coding/.png'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/memory_media_file_storage.py", line 165, in _read_file
    with open(filename, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/root1/wcc/Langchain-Chatchat/coding/.png'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 534, in _run_script
    exec(code, module.__dict__)
  File "/home/root1/wcc/Langchain-Chatchat/webui.py", line 105, in <module>
    pages[selected_page]["func"](api=api, is_lite=is_lite)
  File "/home/root1/wcc/Langchain-Chatchat/webui_pages/dialogue/dialogue.py", line 430, in dialogue_page
    st.image(image_path, caption=" LLM's Image", use_column_width=True)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/metrics_util.py", line 396, in wrapped_func
    result = non_optional_func(*args, **kwargs)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 165, in image
    marshall_images(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 534, in marshall_images
    proto_img.url = image_to_url(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/elements/image.py", line 385, in image_to_url
    url = runtime.get_instance().media_file_mgr.add(image, mimetype, image_id)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/media_file_manager.py", line 222, in add
    file_id = self._storage.load_and_get_id(
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/memory_media_file_storage.py", line 116, in load_and_get_id
    file_data = self._read_file(path_or_data)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/memory_media_file_storage.py", line 168, in _read_file
    raise MediaFileStorageError(f"Error opening '{filename}'") from ex
streamlit.runtime.media_file_storage.MediaFileStorageError: Error opening '/home/root1/wcc/Langchain-Chatchat/coding/.png'
INFO:     127.0.0.1:33508 - "POST /llm_model/get_model_config HTTP/1.1" 200 OK
2024-01-15 20:34:14,271 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/get_model_config "HTTP/1.1 200 OK"
2024-01-15 20:34:14,966 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33520 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:14,969 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:34:15,058 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33520 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:15,061 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:33520 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:34:15,086 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:34:25.466 Uncaught app exception
Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state.py", line 394, in __getitem__
    if widget_id in wid_key_map and widget_id == key:
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state.py", line 439, in _getitem
    except KeyError:
KeyError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state_proxy.py", line 119, in __getattr__
    return self[key]
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state_proxy.py", line 90, in __getitem__
    return get_session_state()[key]
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/safe_session_state.py", line 89, in __getitem__
    self._yield_callback()
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state.py", line 396, in __getitem__
    key = wid_key_map[widget_id]
KeyError: 'st.session_state has no key "dialogue_mode". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 530, in _run_script
    if rerun_data.widget_states is not None:
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/safe_session_state.py", line 61, in on_script_will_rerun
    #  duration. (This will also allow us to downgrade our RLock
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state.py", line 500, in on_script_will_rerun
    # Clear any triggers that weren't reset because the script was disconnected
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state.py", line 513, in _call_callbacks
    wid for wid in self._new_widget_state if self._widget_changed(wid)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state.py", line 260, in call_callback
    kwargs = metadata.callback_kwargs or {}
  File "/home/root1/wcc/Langchain-Chatchat/webui_pages/dialogue/dialogue.py", line 134, in on_mode_change
    mode = st.session_state.dialogue_mode
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/site-packages/streamlit/runtime/state/session_state_proxy.py", line 121, in __getattr__
    raise AttributeError(_missing_attr_error_message(key))
AttributeError: st.session_state has no attribute "dialogue_mode". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization
2024-01-15 20:34:33,980 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51510 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:33,987 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:34:34,224 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51510 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:34,229 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51510 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:34:34,246 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51510 - "POST /llm_model/get_model_config HTTP/1.1" 200 OK
2024-01-15 20:34:37,689 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/get_model_config "HTTP/1.1 200 OK"
2024-01-15 20:34:37,979 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51522 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:37,983 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:34:38,074 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51522 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:38,077 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:51522 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:34:38,090 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
2024-01-15 20:34:49,518 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37022 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:49,523 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
2024-01-15 20:34:49,630 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37022 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:34:49,634 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_running_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37022 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-01-15 20:34:49,658 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/llm_model/list_config_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:37022 - "POST /chat/chat HTTP/1.1" 200 OK
2024-01-15 20:34:49,888 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:7861/chat/chat "HTTP/1.1 200 OK"
2024-01-15 20:34:50,729 - _client.py[line:1729] - INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-15 20:36:49 | INFO | stdout | INFO:     127.0.0.1:54330 - "GET / HTTP/1.1" 404 Not Found
2024-01-15 20:50:30 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346f010>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:50:35 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346e9e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:50:40 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346dcc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:50:45 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4540aac550>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:50:50 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346ed40>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:50:55 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346f1f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:00 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346fa90>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:05 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4540aac370>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:10 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4540aacdc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:15 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346dff0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:20 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346f400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:25 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f454346e3b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:30 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4540aacd00>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:51:35 | ERROR | model_worker | heart beat error: HTTPConnectionPool(host='127.0.0.1', port=20001): Max retries exceeded with url: /receive_heart_beat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4540aac190>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-01-15 20:52:06,145 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:60240 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
2024-01-15 20:52:06,365 - _client.py[line:1013] - INFO: HTTP Request: POST http://127.0.0.1:20001/list_models "HTTP/1.1 200 OK"
INFO:     127.0.0.1:60240 - "POST /llm_model/list_running_models HTTP/1.1" 200 OK
INFO:     127.0.0.1:60240 - "POST /llm_model/list_config_models HTTP/1.1" 200 OK
2024-03-09 10:09:27,318 - startup.py[line:855] - WARNING: Sending SIGKILL to {'zhipu-api': <Process name='api_worker - zhipu-api (355588)' pid=355588 parent=355221 stopped exitcode=-SIGKILL daemon>}
2024-03-09 10:09:27,425 - startup.py[line:855] - WARNING: Sending SIGKILL to {'chatglm3-6b': <Process name='model_worker - chatglm3-6b (355587)' pid=355587 parent=355221 stopped exitcode=-SIGKILL daemon>}
2024-03-09 10:09:27,426 - startup.py[line:855] - WARNING: Sending SIGKILL to <Process name='controller (355467)' pid=355467 parent=355221 stopped exitcode=-SIGKILL daemon>
2024-03-09 10:09:27,508 - startup.py[line:855] - WARNING: Sending SIGKILL to <Process name='openai_api (355586)' pid=355586 parent=355221 stopped exitcode=-SIGKILL daemon>
2024-03-09 10:09:27,509 - startup.py[line:855] - WARNING: Sending SIGKILL to <Process name='API Server (356021)' pid=356021 parent=355221 stopped exitcode=-SIGKILL daemon>
2024-03-09 10:09:27,509 - startup.py[line:855] - WARNING: Sending SIGKILL to <Process name='WEBUI Server (356188)' pid=356188 parent=355221 stopped exitcode=0 daemon>
2024-03-09 10:09:27,509 - startup.py[line:866] - INFO: Process status: {'zhipu-api': <Process name='api_worker - zhipu-api (355588)' pid=355588 parent=355221 stopped exitcode=-SIGKILL daemon>}
2024-03-09 10:09:27,509 - startup.py[line:866] - INFO: Process status: {'chatglm3-6b': <Process name='model_worker - chatglm3-6b (355587)' pid=355587 parent=355221 stopped exitcode=-SIGKILL daemon>}
2024-03-09 10:09:27,509 - startup.py[line:866] - INFO: Process status: <Process name='controller (355467)' pid=355467 parent=355221 stopped exitcode=-SIGKILL daemon>
2024-03-09 10:09:27,509 - startup.py[line:866] - INFO: Process status: <Process name='openai_api (355586)' pid=355586 parent=355221 stopped exitcode=-SIGKILL daemon>
2024-03-09 10:09:27,509 - startup.py[line:866] - INFO: Process status: <Process name='API Server (356021)' pid=356021 parent=355221 stopped exitcode=-SIGKILL daemon>
2024-03-09 10:09:27,510 - startup.py[line:866] - INFO: Process status: <Process name='WEBUI Server (356188)' pid=356188 parent=355221 stopped exitcode=0 daemon>


==============================Langchain-Chatchat Configuration==============================
操作系统：Linux-5.15.0-91-generic-x86_64-with-glibc2.31.
python版本：3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]
项目版本：v0.2.8
langchain版本：0.0.344. fastchat版本：0.2.34


当前使用的分词器：ChineseRecursiveTextSplitter
当前启动的LLM模型：['chatglm3-6b', 'zhipu-api', 'openai-api'] @ cuda
{'device': 'cuda',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'model_path': 'THUDM/chatglm3-6b',
 'port': 20002}
{'api_key': '00f57a85b1e06e2c2d38b3a067698cc0.dTvbnzYTM6afe3M3',
 'device': 'auto',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'online_api': True,
 'port': 21001,
 'provider': 'ChatGLMWorker',
 'version': 'chatglm_turbo',
 'worker_class': <class 'server.model_workers.zhipu.ChatGLMWorker'>}
{'api_base_url': 'https://api.openai.com/v1',
 'api_key': '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',
 'device': 'auto',
 'host': '0.0.0.0',
 'infer_turbo': False,
 'model_name': 'gpt-3.5-turbo',
 'online_api': True,
 'openai_proxy': '',
 'port': 20002}
当前Embbedings模型： bge-large-zh @ cuda


服务端运行信息：
    OpenAI API Server: http://127.0.0.1:20000/v1
    Chatchat  API  Server: http://127.0.0.1:7861
    Chatchat WEBUI Server: http://0.0.0.0:8501
==============================Langchain-Chatchat Configuration==============================


Traceback (most recent call last):
  File "/home/root1/wcc/Langchain-Chatchat/startup.py", line 880, in <module>
    loop.run_until_complete(start_main_server())
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/home/root1/wcc/Langchain-Chatchat/startup.py", line 778, in start_main_server
    cmd = queue.get() # 收到切换模型的消息
  File "<string>", line 2, in get
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/managers.py", line 818, in _callmethod
    kind, result = conn.recv()
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
  File "/home/root1/wcc/Langchain-Chatchat/startup.py", line 608, in f
    raise KeyboardInterrupt(f"{signalname} received")
KeyboardInterrupt: SIGTERM received
/home/root1/anaconda3/envs/langchain/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
